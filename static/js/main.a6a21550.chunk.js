(window.webpackJsonp=window.webpackJsonp||[]).push([[0],{233:function(e,a,t){e.exports=t.p+"static/media/cnn_3.46547ca0.png"},234:function(e,a,t){e.exports=t.p+"static/media/happy.554372ac.png"},235:function(e,a,t){e.exports=t.p+"static/media/surprise.b24d6534.png"},236:function(e,a,t){e.exports=t.p+"static/media/sad.d52cc55f.png"},237:function(e,a,t){e.exports=t.p+"static/media/angry.db0edcfb.png"},238:function(e,a,t){e.exports=t.p+"static/media/fear.ba6177a4.png"},239:function(e,a,t){e.exports=t.p+"static/media/neutral.1848481f.png"},262:function(e,a,t){e.exports=t.p+"static/media/silas.459b2998.jpg"},263:function(e,a,t){e.exports=t.p+"static/media/rama.dcb81d6b.jpg"},264:function(e,a,t){e.exports=t.p+"static/media/maria.1e4f5d7a.png"},265:function(e,a,t){e.exports=t.p+"static/media/UW.6269965e.png"},268:function(e,a,t){e.exports=t(294)},274:function(e,a,t){},281:function(e,a){},282:function(e,a){},290:function(e,a){},291:function(e,a){},292:function(e,a){},293:function(e,a){},294:function(e,a,t){"use strict";t.r(a);var n=t(10),l=t.n(n),r=t(232),i=t.n(r),c=(t(274),t(275),t(71)),o=t(233),s=t.n(o),m=t(158),u=(t(208),t(234)),d=t.n(u),E=t(235),p=t.n(E),h=t(236),g=t.n(h),f=t(237),w=t.n(f),b=t(238),y=t.n(b),v=t(239),N=t.n(v);var C=function(){return l.a.createElement(c.a,{className:"myContainer",id:"ImplementationContainer"},l.a.createElement("h1",{className:"purple"},"Implementation"),l.a.createElement("p",null,"Our implementation was great because..."),l.a.createElement("h3",null,"CNN Architecture"),l.a.createElement("div",{className:"largeImageSection"},l.a.createElement(m.a,null,l.a.createElement("img",{src:s.a,alt:"CNN Architecture",width:800})),l.a.createElement("p",{className:"imageCaption"},"This is a diagram that illustrated the architecture of our cnn model. Click to Zoom.")),l.a.createElement("h3",null,"Emotions"),l.a.createElement("table",null,l.a.createElement("tbody",null,l.a.createElement("tr",null,l.a.createElement("th",null,"Image"),l.a.createElement("th",{className:"cellFullWidth"},"Emotion")),l.a.createElement("tr",null,l.a.createElement("td",null,l.a.createElement("img",{src:d.a,alt:"Happiness",className:"emotionImage"})),l.a.createElement("td",null,"Happy")),l.a.createElement("tr",null,l.a.createElement("td",null,l.a.createElement("img",{src:p.a,alt:"Surprise",className:"emotionImage"})),l.a.createElement("td",null,"Surprise")),l.a.createElement("tr",null,l.a.createElement("td",null,l.a.createElement("img",{src:w.a,alt:"Angry",className:"emotionImage"})),l.a.createElement("td",null,"Anger")),l.a.createElement("tr",null,l.a.createElement("td",null,l.a.createElement("img",{src:g.a,alt:"Sadness",className:"emotionImage"})),l.a.createElement("td",null,"Sad")),l.a.createElement("tr",null,l.a.createElement("td",null,l.a.createElement("img",{src:y.a,alt:"Fear",className:"emotionImage"})),l.a.createElement("td",null,"Fear")),l.a.createElement("tr",null,l.a.createElement("td",null),l.a.createElement("td",null,"Disgust")),l.a.createElement("tr",null,l.a.createElement("td",null,l.a.createElement("img",{src:N.a,alt:"Neutral",className:"emotionImage"})),l.a.createElement("td",null,"Neutral")))))};var I=function(){return l.a.createElement(c.a,{className:"myContainer",id:"ResultsContainer"},l.a.createElement("h1",{className:"orange"},"Results"),l.a.createElement("p",null,"Still in progress!"))},k=(t(12),t(261)),A=t(150);t(203);var x=function(){var e=Object(n.useRef)(null),a=Object(n.useRef)(null);return Object(n.useEffect)(function(){}),l.a.createElement(c.a,{className:"myContainer",id:"StreamContainer"},l.a.createElement("h1",{className:"blue"},"Stream"),l.a.createElement(k.a,null,l.a.createElement(A.a,null,l.a.createElement(m.a,null,l.a.createElement("video",{className:"borderClass",id:"video",width:"640",height:"480",autoPlay:!0,ref:e})),l.a.createElement(m.a,null,l.a.createElement("canvas",{className:"borderClass",id:"canvas",width:"640",height:"480",ref:a}))),l.a.createElement(A.a,null,l.a.createElement("p",null,"Demo still in progress!"))))},M=t(262),S=t.n(M),P=t(263),_=t.n(P),R=t(264),T=t.n(R);var j=function(){return l.a.createElement(c.a,{className:"myContainer",id:"WelcomeContainer"},l.a.createElement("h1",{className:"green"},"Welcome to our website"),l.a.createElement("h2",null,"Meet the team:"),l.a.createElement("div",null,l.a.createElement("h3",null,"Ramapriya Ranganath"),l.a.createElement("div",{className:"welcomeSection"},l.a.createElement("img",{src:_.a,alt:"Picture of Rama",className:"welcomeImage"}),l.a.createElement("p",null,"I am Ramapriya master's student from UW Madison CS Department. Before joining here, I was working in Microsoft Research India. My areas of interest are ML, systems and HCI."))),l.a.createElement("div",null,l.a.createElement("h3",null,"Silas Morris"),l.a.createElement("div",{className:"welcomeSection"},l.a.createElement("img",{src:S.a,alt:"Picture of Silas",className:"welcomeImage"}),l.a.createElement("p",null,"I'm a second year CS Masters student at UW and I'm hoping to graduate in May 2023. I moved to Madison in the summer of 2016 from Conroe, Texas and I'm currently working as a software developer at Epic.")),l.a.createElement("h3",null,"Maria Elisa Montes"),l.a.createElement("div",{className:"welcomeSection"},l.a.createElement("img",{src:T.a,alt:"Picture of Maria",className:"welcomeImage"}),l.a.createElement("p",null,"Hi my name is Maria. I grew up in Guanajuato, Mexico where I got my bachelors in Agronomy Engineering. Last January I came to Madison to start a PhD. in Animal Sciences focusing on precision livestock farming, specifically with dairy cows. In my free time I dance ballet and train gymnastics."))))};var W=function(){return l.a.createElement(c.a,{className:"myContainer pinkContainer",id:"PPContainer"},l.a.createElement("h1",{className:"pink"},"Project proposal"),l.a.createElement("h2",null,"Problem"),l.a.createElement("p",null,"Emotion recognition systems identify features in verbal and non-verbal communication to identify and quantify the emotions expressed (",l.a.createElement("a",{className:"cite",href:"#fragopanagos_2005"},"Fragopanagos & Taylor, 2005"),"; ",l.a.createElement("a",{className:"cite",href:"#zhao_2020"},"Zhao et al., 2020"),"). Data on users' emotional reactions when interacting with online applications is valuable for a wide range of fields education, healthcare, customer service, entertainment (",l.a.createElement("a",{className:"cite",href:"#fragopanagos_2005"},"Fragopanagos & Taylor, 2005"),"), and criminology (",l.a.createElement("a",{className:"cite",href:"podolez_2022"},"Podoletz, 2022"),"). The main signals in emotion analysis are facial expressions, speech, arm gestures, language, and physiological patterns (",l.a.createElement("a",{className:"cite",href:"#zhao_2020"},"Zhao et al., 2020"),"). Emotion recognition from facial expressions involves computer vision, machine learning, and deep learning algorithms. In the context of online applications, the output must be accurate, real-time, and computationally efficient (",l.a.createElement("a",{className:"cite",href:"#hossain_2017"},"Hossain & Muhammad, 2017"),"). Models that analyze images or complete video sequences do not provide an immediate output, therefore, are not suitable for online applications (",l.a.createElement("a",{className:"cite",href:"#hossain_2017"},"Hossain & Muhammad, 2017"),"). Although different frameworks have been proposed, identifying emotions from facial expressions commonly involves face detection, feature extraction, and facial expression classification. In the case of video, dynamic features are extracted using temporal segmentation (",l.a.createElement("a",{className:"cite",href:"#suk_2014"},"Suk & Prabhakaran, 2014"),")."),l.a.createElement("h2",null,"Motivation"),l.a.createElement("p",null,"Automated customer service (",l.a.createElement("a",{className:"cite",href:"#zendesk_2022"},"Zendesk, 2022"),"), hybrid work (",l.a.createElement("a",{className:"cite",href:"#accenture_2021"},"Accenture, 2021"),"), and distance learning ",l.a.createElement("a",{className:"cite",href:"#venable_2022"},"(Venable, 2022)")," are becoming more common, raising the need for natural and personalized interactions with computers. Not only is emotional analysis essential for human-computer interaction but it is also essential for the study of human behavior.  In the gaming industry where, they need real time feedback during the beta testing phase, the company can use our tool to detect the emotions depicted by the testers to get an overall quality of the game. Facial emotion recognition can also be implemented in video from CCTV cameras, providing valuable data for crime prevention and investigation. Because this project will be presented in a web format, we were motivated to focus on emotion analysis for online applications. Our goal is that instructors and anyone that visits our website can interact with our project."),l.a.createElement("h2",null,"Approach"),l.a.createElement("p",null,"Our objective is to develop a program that identifies emotion in real-time video captured through a webcam. We will use an open-source data set to train a deep learning model.  The model uses a cascaded classifier to detect faces in the video frames. The grayscale face region is then passed through the trained model, which classifies facial expressions as anger, disgust, fear, happiness, sadness, surprise, or neutral. This is an already existing framework for emotion recognition which will allow us to compare the performance and accuracy of our implementation with previous work. We will also consider the application of real-time emotion recognition for crime prevention and incorporating object detection. Object detection will not only ensure that emotions are only assigned to persons but also be used for future correlations between objects and emotional states."),l.a.createElement("h2",null,"Milestones"),l.a.createElement("ul",null,l.a.createElement("li",null," ",l.a.createElement("p",null,l.a.createElement("b",null,"Data collection and data assessment.")," Since this project will use an open-source data set, data preparation will not be needed. However, it is essential to assess the quality of the images in the data set. We are looking for a data set with about 3,000 images in each class that considers different ethnicities across emotion categories.")," "),l.a.createElement("li",null,l.a.createElement("b",null,"Program development.")," For this project, we will develop a python code that uses OpenCV, Keras, and TensorFlow libraries to perform real-time facial emotion recognition. It uses a pre-trained deep learning model to predict a person's emotion based on their facial expression captured by a webcam."),l.a.createElement("li",null,l.a.createElement("b",null,"Website and model integration.")," will create a website using React. This website will run an emotion recognition program. The site will request permission to open the webcam. Once the webcam is activated, the video will be displayed on the screen, and a frame indicating the user's emotion will be displayed around the face. Readme instructions on the web page will make it crystal clear to run the code."),l.a.createElement("li",null,l.a.createElement("b",null,"Beta testing")," after completing the requirements for the course project. The team plans to test the program on a practical application and receive feedback from authorities in the field.")),l.a.createElement("h2",null,"Timetable"),l.a.createElement("table",{className:"center_table"},l.a.createElement("tbody",null,l.a.createElement("tr",null,l.a.createElement("th",null,"Type"),l.a.createElement("th",null,"Task"),l.a.createElement("th",null,"Date")),l.a.createElement("tr",null,l.a.createElement("td",null,"Deliverables"),l.a.createElement("td",null,"Project proposal"),l.a.createElement("td",{className:"rightAlign"},"2/19/2023")),l.a.createElement("tr",null,l.a.createElement("td",null),l.a.createElement("td",null,"Mid-term "),l.a.createElement("td",{className:"rightAlign"},"3/02/2023")),l.a.createElement("tr",null,l.a.createElement("td",null),l.a.createElement("td",null,"Final"),l.a.createElement("td",{className:"rightAlign"},"4/30/2023")),l.a.createElement("tr",null,l.a.createElement("td",null,"Model"),l.a.createElement("td",null,"Data collection"),l.a.createElement("td",{className:"rightAlign"},"2/05/2023 ")),l.a.createElement("tr",null,l.a.createElement("td",null),l.a.createElement("td",null,"Data assessment"),l.a.createElement("td",{className:"rightAlign"},"2/05/2023")),l.a.createElement("tr",null,l.a.createElement("td",null),l.a.createElement("td",null,"Preliminary model"),l.a.createElement("td",{className:"rightAlign"},"2/19/2023")),l.a.createElement("tr",null,l.a.createElement("td",null),l.a.createElement("td",null,"Training"),l.a.createElement("td",{className:"rightAlign"},"3/02/2023 ")),l.a.createElement("tr",null,l.a.createElement("td",null),l.a.createElement("td",null,"Evaluation"),l.a.createElement("td",{className:"rightAlign"},"3/02/2023")),l.a.createElement("tr",null,l.a.createElement("td",null),l.a.createElement("td",null,"Implementation"),l.a.createElement("td",{className:"rightAlign"},"3/02/2023")),l.a.createElement("tr",null,l.a.createElement("td",null,"Website"),l.a.createElement("td",null,"Site creation"),l.a.createElement("td",{className:"rightAlign"},"2/19/2023")),l.a.createElement("tr",null,l.a.createElement("td",null),l.a.createElement("td",null,"Content creation"),l.a.createElement("td",{className:"rightAlign"},"3/02/2023")),l.a.createElement("tr",null,l.a.createElement("td",null),l.a.createElement("td",null,"Model integration"),l.a.createElement("td",{className:"rightAlign"},"3/02/2023")),l.a.createElement("tr",null,l.a.createElement("td",null),l.a.createElement("td",null,"Hosting"),l.a.createElement("td",{className:"rightAlign"},"4/22/2023")),l.a.createElement("tr",null,l.a.createElement("td",null),l.a.createElement("td",null,"Web cam function"),l.a.createElement("td",{className:"rightAlign"},"4/22/2023")),l.a.createElement("tr",null,l.a.createElement("td",null),l.a.createElement("td",null,"Complete functionality "),l.a.createElement("td",{className:"rightAlign"},"4/24/2023")))),l.a.createElement("h2",null,"References"),l.a.createElement("ul",null,l.a.createElement("li",{id:"accenture_2021"}," Accenture. (2021). The future of work: Productive anywhere. Retrieved in February 2023 from: ",l.a.createElement("a",{href:"https://www.accenture.com/_acnmedia/PDF-155/Accenture-Future-Of-Work-Global-Report.pdf#zoom=40"},"https://www.accenture.com/_acnmedia/PDF-155/Accenture-Future-Of-Work-Global-Report.pdf#zoom=40 ")),l.a.createElement("li",{id:"fragopanagos_2005"},"Fragopanagos, N., & Taylor, J. G. (2005). Emotion recognition in human\u2013computer interaction. Neural Networks, 18(4), 389\u2013405. ",l.a.createElement("a",{href:"https://doi.org/10.1016/j.neunet.2005.03.006"}," https://doi.org/10.1016/j.neunet.2005.03.006")),l.a.createElement("li",{id:"hossain_2017"},"Hossain, M. S., & Muhammad, G. (2017). An emotion recognition system for mobile applications. IEEE Access (5), 2281-2287. ",l.a.createElement("a",{href:"https://ieeexplore.ieee.org/document/7862118"}," https://ieeexplore.ieee.org/document/7862118 ")),l.a.createElement("li",{id:"podoletz_2022"},"Podoletz, L. (2022). We have to talk about emotional AI and crime. AI & Society. ",l.a.createElement("a",{href:"https://doi.org/10.1007/s00146-022-01435-w"},"https://doi.org/10.1007/s00146-022-01435-w")),l.a.createElement("li",{id:"suk_0214"}," Suk, M., & Prabhakaran, B. (2014). Real-Time Mobile Facial Expression Recognition System -- A Case Study. 2014 IEEE Conference on Computer Vision and Pattern Recognition Workshops. ",l.a.createElement("a",{href:"https://doi:10.1109/cvprw.2014.25"},"https://doi:10.1109/cvprw.2014.25")),l.a.createElement("li",{id:"venable_2022"}," Venable, M. A. (2022). 2022 Online Education Trends Report. BestColleges.com ",l.a.createElement("a",{href:"https://www.bestcolleges.com/research/annual-trends-in-online-education/"},"https://www.bestcolleges.com/research/annual-trends-in-online-education/")),l.a.createElement("li",{id:"zhao_2020"},"Zhao, J., Zhang, A., Rau, P.-L. P., Dong, L., & Ge, L. (2020). Trends in human-computer interaction in the 5G era: Emerging life scenarios with 5G networks. Cross-Cultural Design. User Experience of Products, Services, and Intelligent Environments, 699\u2013710. ",l.a.createElement("a",{href:"https://doi.org/10.1007/978-3-030-49788-0_53"},"https://doi.org/10.1007/978-3-030-49788-0_53")),l.a.createElement("li",{id:"zendesk_2022"},"Zendesk. (2022). CX Trends 2022. Retrieved in February 2023 from: ",l.a.createElement("a",{href:"https://cdn2.assets-servd.host/paltry-coyote/production/exports/1e02568f10207f5f7052a41fa28de0a4/zendesk-cx-trends-2022-report.pdf "}," https://cdn2.assets-servd.host/paltry-coyote/production/exports/1e02568f10207f5f7052a41fa28de0a4/zendesk-cx-trends-2022-report.pdf"))))},F=t(265),z=t.n(F);var D=function(){return l.a.createElement(c.a,{className:"navigatorContainer myContainer",fluid:"false"},l.a.createElement("img",{src:z.a,alt:"UW logo",className:"navImage"}),l.a.createElement("h4",null,l.a.createElement("a",{href:"#WelcomeContainer"},"Welcome")),l.a.createElement("h4",null,l.a.createElement("a",{href:"#PPContainer"},"Project Proposal")),l.a.createElement("h4",null,l.a.createElement("a",{href:"#ImplementationContainer"},"Implementation")),l.a.createElement("h4",null,l.a.createElement("a",{href:"#StreamContainer"},"Stream")),l.a.createElement("h4",null,l.a.createElement("a",{href:"#ResultsContainer"},"Results")))};i.a.createRoot(document.getElementById("root")).render(l.a.createElement(l.a.StrictMode,null,l.a.createElement(c.a,null,l.a.createElement(k.a,null,l.a.createElement(A.a,{xs:6,sm:5,md:4,lg:3},l.a.createElement(D,null)),l.a.createElement(A.a,null,l.a.createElement(j,null),l.a.createElement(W,null),l.a.createElement(C,null),l.a.createElement(x,null),l.a.createElement(I,null))))))}},[[268,1,2]]]);
//# sourceMappingURL=main.a6a21550.chunk.js.map